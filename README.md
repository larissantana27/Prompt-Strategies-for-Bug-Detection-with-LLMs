# Prompt-Strategies-for-Bug-Detection-with-LLMs

# ğŸ EstratÃ©gias de Prompt para DetecÃ§Ã£o de Bugs com LLMs

Trabalho de conclusÃ£o da disciplina **ManutenÃ§Ã£o e EvoluÃ§Ã£o de Software (MES)**  
Programa de PÃ³s-GraduaÃ§Ã£o em CiÃªncia da ComputaÃ§Ã£o â€” UFMG  
Autores: Ester de Lima Pontes Andrade  e Larissa Duarta Santana

---

## ğŸ¯ Objetivo

Este repositÃ³rio contÃ©m um estudo experimental que investiga o impacto da **engenharia de prompts** na performance de modelos de linguagem (LLMs) para detectar **bugs sintÃ¡ticos** em grandes arquivos de cÃ³digo Python. Para isso, utilizamos o benchmark **Bug In the Code Stack (BICS)** e propomos tambÃ©m **novas visualizaÃ§Ãµes** para anÃ¡lise dos resultados.

Nosso foco estÃ¡ na melhoria da interpretabilidade e controle das ferramentas de manutenÃ§Ã£o automatizada baseadas em LLMs.

---

## ğŸ“¦ Metodologia

### ğŸ§  Modelos de Linguagem Utilizados
Este estudo utilizarÃ¡ modelos de linguagem jÃ¡ avaliados no artigo original do benchmark BICS, a fim de manter consistÃªncia metodolÃ³gica e permitir comparaÃ§Ãµes diretas:

- `GPT-4o (OpenAI)`
- `Claude 3.5 (Anthropic)` 
- `Gemini 1.5 Pro (Google)`
- `LLaMA3-70B (Meta, open-source)`

Esses modelos serÃ£o acessados via LiteLLM API (como no benchmark original), ou diretamente por API oficial, conforme disponibilidade.

#### ğŸ” Possibilidade adicional: 

Se houver tempo e recursos computacionais disponÃ­veis, tambÃ©m realizaremos testes com modelos open-source como:

- `Mistral-7B-Instruct`
- `CodeLLaMA-13B`

Esses modelos serÃ£o executados localmente para fins de comparaÃ§Ã£o e reprodutibilidade.


### ğŸ“š Dataset Utilizado
- Benchmark **BICS**, baseado no [Python Alpaca Dataset](https://huggingface.co/datasets/tarunbisht/python-code-instructions-18k-alpaca)
- ContÃ©m cÃ³digo realista de vÃ¡rias Ã¡reas: ML, banco de dados, algoritmos
- Tipos de bug incluÃ­dos:
  - Falta de dois pontos, colchetes trocados, uso incorreto de aspas, etc.

### ğŸ”„ Engenharia de Prompt
Testaremos variaÃ§Ãµes como:
- **Zero-shot** (sem exemplo anterior)
- **Two-shot** (com exemplos, como no benchmark original)

Essas variaÃ§Ãµes ajudam a entender como a **formulaÃ§Ã£o da instruÃ§Ã£o** afeta o desempenho do modelo.

---

## ğŸ“Š AvaliaÃ§Ã£o

A avaliaÃ§Ã£o serÃ¡ dividida em duas partes: **quantitativa** (mÃ©trica de acerto) e **qualitativa** (anÃ¡lise manual das respostas), com foco em interpretar o desempenho dos modelos em diferentes variaÃ§Ãµes de prompt.

---

### ğŸ“ˆ AvaliaÃ§Ã£o Quantitativa

Nesta parte, vamos medir o desempenho dos modelos com base em sua capacidade de **localizar corretamente bugs sintÃ¡ticos no cÃ³digo**.

#### ğŸ”¹ MÃ©trica principal:
- **AcurÃ¡cia**: porcentagem de vezes que o modelo acerta:
  - A **linha correta** onde o bug estÃ¡ inserido.
  - O **tipo de bug** (entre os 7 tipos definidos pelo benchmark BICS).

#### ğŸ”¹ ComparaÃ§Ãµes previstas:
- Desempenho entre diferentes **modelos de linguagem** (ex: GPT-4o, Claude 3.5, LLaMA3-70B).
- Desempenho entre diferentes **variaÃ§Ãµes de prompt** (zero-shot e two-shot).

#### ğŸ”¹ VisualizaÃ§Ãµes que serÃ£o utilizadas:
- **GrÃ¡ficos de barras**: para comparar a acurÃ¡cia mÃ©dia por modelo e por tipo de prompt.
- **GrÃ¡ficos por contexto (nÃºmero de tokens)**: para observar se o tamanho do cÃ³digo influencia a precisÃ£o dos modelos.

> As visualizaÃ§Ãµes serÃ£o geradas com ferramentas como Matplotlib ou Seaborn, priorizando representaÃ§Ãµes simples e claras.

---

### ğŸ§ AvaliaÃ§Ã£o Qualitativa

Nesta etapa, poderÃ¡ ser feita uma anÃ¡lise manual/automÃ¡tica de exemplos selecionados, com o objetivo de observar **como os modelos se comportam alÃ©m dos nÃºmeros**.

As respostas dos modelos (linha e tipo de bug) jÃ¡ sÃ£o salvas automaticamente no benchmark, o que permite revisar e selecionar exemplos manualmente para anÃ¡lise qualitativa, onde pode ser classificados como:

- Casos onde o modelo acerta, mas com saÃ­da incompleta ou genÃ©rica.
- Casos em que o modelo erra, mas a saÃ­da parece plausÃ­vel.
- Casos em que o modelo retorna algo fora do esperado.

> A anÃ¡lise qualitativa Ã© importante para avaliar a **usabilidade prÃ¡tica** dos modelos em tarefas reais de manutenÃ§Ã£o de software.

---

## ğŸ“ OrganizaÃ§Ã£o do RepositÃ³rio

---

## ğŸ“š ReferÃªncias

- Lee, H., Sharma, S., & Hu, B. (2024). Bug In The Code Stack: Can LLMs Find Bugs In Large Python Code Stacks? [arXiv:2406.15325](https://arxiv.org/abs/2406.15325)
- Tarun Bisht. python-code-instructions-18k-alpaca. HuggingFace Dataset. [https://huggingface.co/datasets/tarunbisht/python-code-instructions-18k-alpaca](https://huggingface.co/datasets/tarunbisht/python-code-instructions-18k-alpaca)
- Benchmark oficial: [https://github.com/HammingHQ/bug-in-the-code-stack](https://github.com/HammingHQ/bug-in-the-code-stack)

